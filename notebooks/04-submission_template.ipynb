{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import PosixPath\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5507c53e9a84fa9873ebf5349da5f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_dir = '../data/models/fine_tuned/'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    model_dir, \n",
    "    max_memory={\"cpu\": \"1GIB\"}, \n",
    "    offload_state_dict=True, \n",
    "    # dtorch_device=device,\n",
    "    torch_dtype=torch.float16).to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(image, pre_prompt=None, dtype=torch.float16):\n",
    "    inputs = processor(image, text=pre_prompt, return_tensors=\"pt\").to(device, dtype)\n",
    "\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128, num_beams=3)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_inference(images, pre_prompt=None, dtype=torch.float16):\n",
    "    inputs = processor(images, text=pre_prompt, return_tensors=\"pt\").to(device, dtype)\n",
    "\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128, num_beams=3)\n",
    "    generated_text = [item.strip() for item in processor.batch_decode(generated_ids, skip_special_tokens=True)]\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Setup inference images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = PosixPath('../data/images')\n",
    "images = data_dir.glob('**/*.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, path: str):\n",
    "        self.path = PosixPath(path)\n",
    "        self.images = list(self.path.glob('**/*.png'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        image_path = self.images[idx]\n",
    "        return Image.open(image_path), image_path.name.split('.')[0]\n",
    "    \n",
    "def collate_fn(data):\n",
    "    batch = {'images': [], 'ids': []}\n",
    "    for row in data:\n",
    "        batch['images'].append(row[0])\n",
    "        batch['ids'].append(row[1])\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = ImageDataset('../data/images')\n",
    "image_loader = DataLoader(image_dataset, shuffle=False, batch_size=4, pin_memory=True, collate_fn=collate_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Run image2prompt inference on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "row_idx = -1\n",
    "prompt_df = pd.DataFrame(columns=['imgId', 'prompt'])\n",
    "with torch.no_grad():\n",
    "    for batch in image_loader:\n",
    "        # batch = next(iter(image_loader))\n",
    "        prompts = batch_inference(batch['images'])\n",
    "        for idx, prompt in enumerate(prompts):\n",
    "            row_idx += 1\n",
    "            img_id = batch['ids'][idx]\n",
    "            prompt_df.loc[row_idx] = [img_id, prompt]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imgId</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20057f34d</td>\n",
       "      <td>a black hole in the center of the earth, by mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>227ef0887</td>\n",
       "      <td>wooden sculpture, intricate detail, 8 k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92e911621</td>\n",
       "      <td>a dinosaur eating a cheese pizza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a4e1c55a9</td>\n",
       "      <td>a drawing of a robot, in a style of a cartoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c98f79f71</td>\n",
       "      <td>portrait of a man in a dinosaur costume, by gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>d8edf2e40</td>\n",
       "      <td>an astronaut in a space suit standing in front...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>f27825b2c</td>\n",
       "      <td>a donut in a donut shop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       imgId                                             prompt\n",
       "0  20057f34d  a black hole in the center of the earth, by mi...\n",
       "1  227ef0887            wooden sculpture, intricate detail, 8 k\n",
       "2  92e911621                   a dinosaur eating a cheese pizza\n",
       "3  a4e1c55a9      a drawing of a robot, in a style of a cartoon\n",
       "4  c98f79f71  portrait of a man in a dinosaur costume, by gr...\n",
       "5  d8edf2e40  an astronaut in a space suit standing in front...\n",
       "6  f27825b2c                            a donut in a donut shop"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Load sentence embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 384\n",
    "\n",
    "def embed_prompts(prompts_df: pd.DataFrame) -> np.ndarray:\n",
    "    return embedding_model.encode(prompts_df['prompt'])\n",
    "\n",
    "def build_submission(prompts_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    prompt_embedding = embed_prompts(prompts_df)\n",
    "    output_df = pd.DataFrame(columns=['imgId_eId', 'val'])\n",
    "    \n",
    "    for prompt_idx, _ in enumerate(prompts_df.prompt):\n",
    "        imgId = prompts_df.iloc[prompt_idx].imgId\n",
    "        embed_vec = prompt_embedding[prompt_idx]\n",
    "\n",
    "        for embed_idx, val in enumerate(embed_vec):\n",
    "            row_idx = (embedding_size * prompt_idx) + embed_idx\n",
    "            output_df.loc[row_idx] = (f'{imgId}_{embed_idx}', val)\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_submission_df = build_submission(prompt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imgId_eId</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20057f34d_0</td>\n",
       "      <td>-0.023036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20057f34d_1</td>\n",
       "      <td>0.021516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20057f34d_2</td>\n",
       "      <td>0.010190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20057f34d_3</td>\n",
       "      <td>0.064952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20057f34d_4</td>\n",
       "      <td>0.015292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2683</th>\n",
       "      <td>f27825b2c_379</td>\n",
       "      <td>0.001352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2684</th>\n",
       "      <td>f27825b2c_380</td>\n",
       "      <td>-0.061341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2685</th>\n",
       "      <td>f27825b2c_381</td>\n",
       "      <td>-0.027693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2686</th>\n",
       "      <td>f27825b2c_382</td>\n",
       "      <td>0.035682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2687</th>\n",
       "      <td>f27825b2c_383</td>\n",
       "      <td>-0.030249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2688 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          imgId_eId       val\n",
       "0       20057f34d_0 -0.023036\n",
       "1       20057f34d_1  0.021516\n",
       "2       20057f34d_2  0.010190\n",
       "3       20057f34d_3  0.064952\n",
       "4       20057f34d_4  0.015292\n",
       "...             ...       ...\n",
       "2683  f27825b2c_379  0.001352\n",
       "2684  f27825b2c_380 -0.061341\n",
       "2685  f27825b2c_381 -0.027693\n",
       "2686  f27825b2c_382  0.035682\n",
       "2687  f27825b2c_383 -0.030249\n",
       "\n",
       "[2688 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_submission_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
